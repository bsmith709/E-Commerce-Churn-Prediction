{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd8dc517",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis for Olist Ecommerce Dataset\n",
    "This notebook will explore the Olist Ecommerce Dataset to identify key characteristics useful for predicting customer churn.\n",
    "\n",
    "First, import libraries and configure dataset directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2d7713",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "DATA_DIR = 'archive'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97054650",
   "metadata": {},
   "source": [
    "Load each dataset as a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1af026",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading datasets...\")\n",
    "\n",
    "try:\n",
    "    customers_df = pd.read_csv(os.path.join(DATA_DIR, 'olist_customers_dataset.csv'))\n",
    "    orders_df = pd.read_csv(os.path.join(DATA_DIR, 'olist_orders_dataset.csv'))\n",
    "    order_items_df = pd.read_csv(os.path.join(DATA_DIR, 'olist_order_items_dataset.csv'))\n",
    "    order_payments_df = pd.read_csv(os.path.join(DATA_DIR, 'olist_order_payments_dataset.csv'))\n",
    "    # product_info needs joining orders->items->products later\n",
    "    products_df = pd.read_csv(os.path.join(DATA_DIR, 'olist_products_dataset.csv'))\n",
    "    # reviews might be useful for features later\n",
    "    reviews_df = pd.read_csv(os.path.join(DATA_DIR, 'olist_order_reviews_dataset.csv'))\n",
    "\n",
    "    print(\"Datasets loaded successfully.\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading datasets: {e}\")\n",
    "    print(f\"Please ensure the CSV files are in the '{DATA_DIR}' directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99632840",
   "metadata": {},
   "source": [
    "Get a first look at each dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41db3690",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = {\n",
    "    \"Customers\": customers_df,\n",
    "    \"Orders\": orders_df,\n",
    "    \"Order Items\": order_items_df,\n",
    "    \"Order Payments\": order_payments_df,\n",
    "    \"Products\": products_df,\n",
    "    \"Reviews\": reviews_df\n",
    "}\n",
    "\n",
    "for name, df in dataframes.items():\n",
    "    print(f\"\\n--- Inspecting: {name} DataFrame ---\")\n",
    "    print(f\"Shape: {df.shape}\") # (rows, columns)\n",
    "\n",
    "    print(\"\\nFirst 5 rows:\")\n",
    "    # Display more columns if needed: pd.set_option('display.max_columns', None)\n",
    "    print(df.head())\n",
    "\n",
    "    print(\"\\nInfo (Data Types & Non-Null Counts):\")\n",
    "    # This is crucial for spotting missing values and wrong data types\n",
    "    df.info()\n",
    "\n",
    "    # Get basic statistics for numerical columns, only if they exist\n",
    "    #print(\"\\nDescriptive Statistics (Numerical Columns):\")\n",
    "    #numerical_cols = df.select_dtypes(include=np.number).columns\n",
    "    #if not numerical_cols.empty:\n",
    "    #    print(df.describe(include=[np.number])) # Use include=[np.number] to only show numerical stats initially\n",
    "    #else:\n",
    "    #    print(\"No numerical columns found in this DataFrame.\")\n",
    "\n",
    "\n",
    "    # Optional: Look at categorical descriptions if needed later\n",
    "    # print(\"\\nDescriptive Statistics (Categorical Columns):\")\n",
    "    # print(df.describe(include=['object']))\n",
    "\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89670ad",
   "metadata": {},
   "source": [
    "### Most Important Features Identified\n",
    "- #### Recency: Days since customers last purchase\n",
    "    - **Reasoning:**  Customers who haven't bought in a long time are likely to have churned\n",
    "    - **Extraction:** Max of order_purchase_timestamp\n",
    "- #### Frequency: Total number of orders a customer has placed\n",
    "    - **Reasoning:** Customers who buy often are more engaged, less likely to churn\n",
    "    - **Extraction:** Count unique order_ids associated with customer\n",
    "- #### Tenure: Days between customers first and last purchase\n",
    "    - **Reasoning:** Helps identify low-risk, loyal customers that are less likely to churn\n",
    "    - **Extraction:** Difference of last purchase and first purchase in days \n",
    "- #### Monetary: Total amount of money customer has spent\n",
    "    - **Reasoning:** High-spenders are valuable, and their spending habit is a strong engagement signal\n",
    "    - **Extraction:** Sum of all order payment values associated with customer\n",
    "- #### Average Payment Installments\n",
    "    - **Reasoning:** Customers who pay in many installments likely place high value orders and experience different churn patterns\n",
    "    - **Extraction:** Average of all payment installment amounts for orders associated with customer\n",
    "- #### Preferred Payment Method\n",
    "    - **Reasoning:** Customers may have different curn patterns based on preferred payment type\n",
    "    - **Extraction:** Mode of all payment_type associated with customer\n",
    "- #### Average Payment Complexity: Unique payment methods per single order\n",
    "    - **Reasoning:** Customers who use multiple payment methods per order likely wait for vouchers and have a different churn profile\n",
    "    - **Extraction:** Average of the max payment_sequential for each order associated with a customer\n",
    "- #### Average Review Score\n",
    "    - **Reasoning:** Customers who leave high reviews are more satisfied, less likely to churn\n",
    "    - **Extraction:** Average of all reviews associated with customer\n",
    "- #### Review Engagement Rate: How often does customer leave review messages\n",
    "    - **Reasoning:** A customer who leaves review messages, good or bad, is more engaged and less likely to churn\n",
    "    - **Extraction:** The ratio of orders with messages vs orders without messages for orders associated wtih customer\n",
    "- #### Average Shipping Cost\n",
    "    - **Reasoning:** Customers who pay more for shipping are more likely to churn\n",
    "    - **Extraction:** Take the average sum of freight_value for every order item for orders associated with customer\n",
    "- #### Number of Unique Product Categories Purchased\n",
    "    - **Reasoning:** Customers who have bought products from more categories are more invested in the platform, less likely to churn\n",
    "    - **Extraction:** Number of unique product categories associated with customer orders\n",
    "- #### Number of Unique Sellers Purchased From\n",
    "    - **Reasoning:** Customers who purchase form many sellers are more engaged, less likely to churn\n",
    "    - **Extraction:** Total of unique seller_id for orders items associated with customer\n",
    "- #### Average Payment Approval Time: Gap between order placement and approval\n",
    "    - **Reasoning:** Customers who frequently experience delays between placing orders and order approval are more likely to churn\n",
    "    - **Extraction:** Average difference between order approval and order placement\n",
    "- #### Average Delivery vs Estimate\n",
    "    - **Reasoning:** Customers who consistently get late packages are likely to churn\n",
    "    - **Extraction:** Average time between order_delivered_customer_date and order_estimated_delivery_date\n",
    "- #### Average Carrier Transit Time\n",
    "    - **Reasoning:** Customers who experience long shipping times are more likely to churn\n",
    "    - **Extraction:** Average difference between carier delivery and customer delivery of all orders associated with a customer\n",
    "- #### Average Time Between Seller Shipping Deadline and Carrier Delivery\n",
    "    - **Reasoning:** Sellers not meeting their fulfillment deadlines will make customers more liekly to churn\n",
    "    - **Extraction:** Average difference between seller deadline and carrier delivery for orders associated with customer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed68a42",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "- The previous output shows that dates are stored as strings, they will need to be converted to datetime objects. \n",
    "- There are some missing orders that will need filtered out. \n",
    "- Products with missing categories will need to have their categories set to 'unknown'. \n",
    "- Can remove customer_zip_code_prefix, customer_city, customer_state, order_item_id, review_id, review_comment_title, all product columns except product_id and product_category.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675591fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Clean and Minimize Orders DataFrame ---\n",
    "try:\n",
    "    print(f\"Original shape: {orders_df.shape}\")\n",
    "\n",
    "    # Investigate 'order_status'\n",
    "    print(\"\\n'order_status' counts:\")\n",
    "    print(orders_df['order_status'].value_counts(dropna=False))\n",
    "\n",
    "    # Filter for 'delivered' orders\n",
    "    delivered_orders_df = orders_df[orders_df['order_status'] == 'delivered'].copy()\n",
    "    print(f\"\\nFiltered for 'delivered' status. New shape: {delivered_orders_df.shape}\")\n",
    "\n",
    "    # Convert Timestamps\n",
    "    print(\"\\nConverting timestamp columns...\")\n",
    "    timestamp_cols = [\n",
    "        'order_purchase_timestamp',\n",
    "        'order_approved_at',\n",
    "        'order_delivered_carrier_date',\n",
    "        'order_delivered_customer_date',\n",
    "        'order_estimated_delivery_date'\n",
    "    ]\n",
    "    for col in timestamp_cols:\n",
    "        delivered_orders_df[col] = pd.to_datetime(delivered_orders_df[col])\n",
    "    \n",
    "    # --- Create a Minimal DataFrame for Merging ---\n",
    "    cleaned_orders_minimal = delivered_orders_df[['order_id','customer_id','order_purchase_timestamp','order_approved_at','order_delivered_carrier_date','order_delivered_customer_date','order_estimated_delivery_date']]\n",
    "\n",
    "    # Verify changes\n",
    "    print(\"\\nVerifying 'cleaned_orders_minimal' Dtypes:\")\n",
    "    cleaned_orders_minimal.info()\n",
    "\n",
    "    # Save the cleaned file\n",
    "    output_file_orders = f'{DATA_DIR}/cleaned_orders_minimal.csv'\n",
    "    cleaned_orders_minimal.to_csv(output_file_orders, index=False)\n",
    "    print(f\"\\nSuccessfully saved cleaned minimal orders data to: {output_file_orders}\")\n",
    "    dataframes[\"Orders\"] = cleaned_orders_minimal  # Update the reference\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred: {e}\")\n",
    "\n",
    "\n",
    "# --- 2. Clean and Minimize Reviews DataFrame ---\n",
    "print(\"\\n--- Processing: Reviews DataFrame ---\")\n",
    "try:\n",
    "    print(f\"Original shape: {reviews_df.shape}\")\n",
    "    print(\"Converting timestamp columns...\")\n",
    "\n",
    "    review_ts_cols = ['review_creation_date', 'review_answer_timestamp']\n",
    "    for col in review_ts_cols:\n",
    "        reviews_df[col] = pd.to_datetime(reviews_df[col])\n",
    "\n",
    "    cleaned_reviews_minimal = reviews_df[['order_id','review_score', 'review_comment_message']]\n",
    "    \n",
    "    # Verify changes\n",
    "    print(\"\\nVerifying 'cleaned_reviewes_minimal' Dtypes:\")\n",
    "    cleaned_reviews_minimal.info()\n",
    "\n",
    "    # Save the cleaned file\n",
    "    output_file_reviews = f'{DATA_DIR}/cleaned_reviews_minimal.csv'\n",
    "    cleaned_reviews_minimal.to_csv(output_file_reviews, index=False)\n",
    "    print(f\"\\nSuccessfully saved cleaned minimal reviews data to: {output_file_reviews}\")\n",
    "    dataframes[\"Reviews\"] = cleaned_reviews_minimal  # Update the reference\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred: {e}\")\n",
    "\n",
    "\n",
    "# --- 3. Clean and Minimize Order Items DataFrame ---\n",
    "print(\"\\n--- Processing: Order Items DataFrame ---\")\n",
    "try:\n",
    "    print(f\"Original shape: {order_items_df.shape}\")\n",
    "    print(\"Converting 'shipping_limit_date' to datetime object...\")\n",
    "    \n",
    "    order_items_df['shipping_limit_date'] = pd.to_datetime(order_items_df['shipping_limit_date'])\n",
    "\n",
    "    cleaned_order_items_minimal = order_items_df[['order_id','product_id','seller_id','shipping_limit_date','price','freight_value']]\n",
    "    \n",
    "    # Verify changes\n",
    "    print(\"\\nVerifying 'cleaned_order_items_minimal' Dtypes:\")\n",
    "    cleaned_order_items_minimal.info()\n",
    "\n",
    "    # Save the cleaned file\n",
    "    output_file_items = f'{DATA_DIR}/cleaned_order_items_minimal.csv'\n",
    "    cleaned_order_items_minimal.to_csv(output_file_items, index=False)\n",
    "    print(f\"\\nSuccessfully saved cleaned minimal order items data to: {output_file_items}\")\n",
    "    dataframes[\"Order Items\"] = cleaned_order_items_minimal  # Update the reference\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred: {e}\")\n",
    "\n",
    "# --- 4. Clean and Minimize Products DataFrame ---\n",
    "try:\n",
    "    print(f\"Original shape: {products_df.shape}\")\n",
    "\n",
    "    # --- 1. Inspect Missing Values (Before) ---\n",
    "    missing_count = products_df['product_category_name'].isnull().sum()\n",
    "    print(f\"\\nMissing 'product_category_name' values (Before): {missing_count}\")\n",
    "\n",
    "    # --- 2. Clean the Column ---\n",
    "    # Fill NaN values with the string 'unknown'\n",
    "    products_df['product_category_name'] = products_df['product_category_name'].fillna('unknown')\n",
    "    print(\"Filled NaN values with 'unknown'.\")\n",
    "\n",
    "    # --- 3. Verify the Cleaning (After) ---\n",
    "    missing_count_after = products_df['product_category_name'].isnull().sum()\n",
    "    print(f\"Missing 'product_category_name' values (After): {missing_count_after}\")\n",
    "\n",
    "    # --- 4. Create a Minimal DataFrame for Merging ---\n",
    "    cleaned_products_minimal = products_df[['product_id', 'product_category_name']]\n",
    "    \n",
    "    print(\"\\nHead of the cleaned, minimal products DataFrame:\")\n",
    "    print(cleaned_products_minimal.head())\n",
    "\n",
    "    # --- 5. Save the Cleaned File ---\n",
    "    output_file = f'{DATA_DIR}/cleaned_products_minimal.csv'\n",
    "    cleaned_products_minimal.to_csv(output_file, index=False)\n",
    "    print(f\"\\nSuccessfully saved cleaned minimal product data to: {output_file}\")\n",
    "    dataframes[\"Products\"] = cleaned_products_minimal  # Update the reference\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred: {e}\")\n",
    "\n",
    "# --- 5. Minimize Customer DataFrame ---\n",
    "try:\n",
    "    print(f\"Original shape: {customers_df.shape}\")\n",
    "\n",
    "    # --- 1. Create a Minimal DataFrame for Merging ---\n",
    "    customers_minimal = customers_df[['customer_id', 'customer_unique_id']]\n",
    "    \n",
    "    print(\"\\nHead of the minimal customer DataFrame:\")\n",
    "    print(customers_minimal.head())\n",
    "\n",
    "    # --- 2. Save the Minimal File ---\n",
    "    output_file = f'{DATA_DIR}/customers_minimal.csv'\n",
    "    customers_minimal.to_csv(output_file, index=False)\n",
    "    print(f\"\\nSuccessfully saved minimal customer data to: {output_file}\")\n",
    "    dataframes[\"Customers\"] = customers_minimal  # Update the reference\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred: {e}\")\n",
    "\n",
    "print(\"\\n--- All cleaning tasks complete. ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75124498",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "Now that the dataframes are cleaned and minimized, it is time to calculate features and assemble the final customer centric dataframe. We'll start by calculating the recency, frequency, and tenure features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d675f464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join customers and orders to link customer_unique_id to order_id\n",
    "dataframes[\"Customer Orders\"] = pd.merge(\n",
    "    dataframes[\"Customers\"],  # Has customer_id, customer_unique_id\n",
    "    dataframes[\"Orders\"],     # Has customer_id, order_id, and all dates\n",
    "    on='customer_id'\n",
    ")\n",
    "\n",
    "# We need a \"snapshot date\" to calculate recency (1 day after the last purchase)\n",
    "snapshot_date = dataframes[\"Customer Orders\"]['order_purchase_timestamp'].max() + pd.Timedelta(days=1)\n",
    "\n",
    "# Group by customer and aggregate\n",
    "rft_features = dataframes[\"Customer Orders\"].groupby('customer_unique_id').agg(\n",
    "    last_purchase=('order_purchase_timestamp', 'max'),\n",
    "    Frequency=('order_id', 'nunique'),\n",
    "    first_purchase=('order_purchase_timestamp', 'min')\n",
    ").reset_index()\n",
    "\n",
    "# Calculate Recency and Tenure in days\n",
    "rft_features['Recency'] = (snapshot_date - rft_features['last_purchase']).dt.days\n",
    "rft_features['Tenure'] = (rft_features['last_purchase'] - rft_features['first_purchase']).dt.days\n",
    "\n",
    "# This is our starting point for the final DataFrame\n",
    "dataframes[\"Customer Centric\"] = rft_features[['customer_unique_id', 'Recency', 'Frequency', 'Tenure']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f29478",
   "metadata": {},
   "source": [
    "Now moving on to the monetary and payment features..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3581a61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Link payments to the customer/order master\n",
    "dataframes[\"Merged Payments\"] = pd.merge(\n",
    "    dataframes[\"Customer Orders\"][['customer_unique_id', 'order_id']],\n",
    "    dataframes[\"Order Payments\"],\n",
    "    on='order_id'\n",
    ")\n",
    "\n",
    "# 1. Aggregate basic payment features\n",
    "payment_features = dataframes[\"Merged Payments\"].groupby('customer_unique_id').agg(\n",
    "    Monetary=('payment_value', 'sum'),\n",
    "    avg_payment_installments=('payment_installments', 'mean'),\n",
    "    preferred_payment_method=('payment_type', lambda x: x.mode()[0]) # Get the most frequent\n",
    ").reset_index()\n",
    "\n",
    "# 2. Calculate Payment Complexity (multi-step)\n",
    "# Find the max sequential number for *each order*\n",
    "order_complexity = dataframes[\"Merged Payments\"].groupby(['customer_unique_id', 'order_id']) \\\n",
    "                                  .agg(max_sequential=('payment_sequential', 'max')) \\\n",
    "                                  .reset_index()\n",
    "# Now, find the average complexity *per customer*\n",
    "customer_complexity = order_complexity.groupby('customer_unique_id') \\\n",
    "                                      .agg(avg_payment_complexity=('max_sequential', 'mean')) \\\n",
    "                                      .reset_index()\n",
    "\n",
    "# --- Merge this group into the main DataFrame ---\n",
    "dataframes[\"Customer Centric\"] = pd.merge(dataframes[\"Customer Centric\"], payment_features, on='customer_unique_id', how='left')\n",
    "dataframes[\"Customer Centric\"] = pd.merge(dataframes[\"Customer Centric\"], customer_complexity, on='customer_unique_id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168f8d6e",
   "metadata": {},
   "source": [
    "Next review features..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f30956",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_merged = pd.merge(\n",
    "    dataframes[\"Customer Orders\"][['customer_unique_id', 'order_id']],\n",
    "    reviews_df,\n",
    "    on='order_id'\n",
    ")\n",
    "\n",
    "# Aggregate review features\n",
    "review_features = reviews_merged.groupby('customer_unique_id').agg(\n",
    "    avg_review_score=('review_score', 'mean'),\n",
    "    # .mean() on a boolean (notnull()) gives the ratio/rate\n",
    "    review_engagement_rate=('review_comment_message', lambda x: x.notnull().mean()) \n",
    ").reset_index()\n",
    "\n",
    "# --- Merge this group into the main DataFrame ---\n",
    "dataframes[\"Customer Centric\"] = pd.merge(dataframes[\"Customer Centric\"], review_features, on='customer_unique_id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f4ece6",
   "metadata": {},
   "source": [
    "Item, product, and seller features..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bb1f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Link items to customers/orders\n",
    "items_merged = pd.merge(\n",
    "    dataframes[\"Customer Orders\"][['customer_unique_id', 'order_id']],\n",
    "    dataframes[\"Order Items\"],\n",
    "    on='order_id'\n",
    ")\n",
    "# Now link to products to get the category\n",
    "products_merged = pd.merge(\n",
    "    items_merged,\n",
    "    products_df,\n",
    "    on='product_id'\n",
    ")\n",
    "\n",
    "# Aggregate features from this combined table\n",
    "item_features = products_merged.groupby('customer_unique_id').agg(\n",
    "    total_freight_value=('freight_value', 'sum'), # We'll calculate avg cost later\n",
    "    unique_product_categories=('product_category_name', 'nunique'),\n",
    "    unique_sellers=('seller_id', 'nunique')\n",
    ").reset_index()\n",
    "\n",
    "# --- Merge this group into the main DataFrame ---\n",
    "dataframes[\"Customer Centric\"] = pd.merge(dataframes[\"Customer Centric\"], item_features, on='customer_unique_id', how='left')\n",
    "\n",
    "# Create the Average Shipping Cost (using total_freight and Frequency)\n",
    "dataframes[\"Customer Centric\"]['avg_shipping_cost'] = (\n",
    "    dataframes[\"Customer Centric\"].pop('total_freight_value') / dataframes[\"Customer Centric\"]['Frequency']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fa16f8",
   "metadata": {},
   "source": [
    "Gap time features..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a146b23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to link items to get the shipping_limit_date for each order\n",
    "gaps_merged = pd.merge(\n",
    "    dataframes[\"Customer Orders\"],\n",
    "    dataframes[\"Order Items\"][['order_id', 'shipping_limit_date']],\n",
    "    on='order_id'\n",
    ")\n",
    "\n",
    "# De-duplicate: An order with 3 items will have 3 rows. We only need one row per order.\n",
    "# We'll take the LAST shipping_limit_date as the deadline for the whole order.\n",
    "gaps_df_orders = gaps_merged.groupby('order_id').agg({\n",
    "    'customer_unique_id': 'first',\n",
    "    'order_purchase_timestamp': 'first',\n",
    "    'order_approved_at': 'first',\n",
    "    'order_delivered_carrier_date': 'first',\n",
    "    'order_delivered_customer_date': 'first',\n",
    "    'order_estimated_delivery_date': 'first',\n",
    "    'shipping_limit_date': 'max' # Use the last deadline\n",
    "}).reset_index()\n",
    "\n",
    "# Calculate all gaps in hours (or days)\n",
    "gaps_df_orders['avg_payment_approval_time'] = (gaps_df_orders['order_approved_at'] - gaps_df_orders['order_purchase_timestamp']).dt.total_seconds() / 3600\n",
    "gaps_df_orders['avg_delivery_vs_estimate'] = (gaps_df_orders['order_estimated_delivery_date'] - gaps_df_orders['order_delivered_customer_date']).dt.total_seconds() / (24 * 3600)\n",
    "gaps_df_orders['avg_carrier_shipping_time'] = (gaps_df_orders['order_delivered_customer_date'] - gaps_df_orders['order_delivered_carrier_date']).dt.total_seconds() / (24 * 3600)\n",
    "gaps_df_orders['avg_shipping_vs_deadline'] = (gaps_df_orders['shipping_limit_date'] - gaps_df_orders['order_delivered_carrier_date']).dt.total_seconds() / (24 * 3600)\n",
    "\n",
    "# Now, average these order-level gaps for each customer\n",
    "gap_features = gaps_df_orders.groupby('customer_unique_id')[[\n",
    "    'avg_payment_approval_time', 'avg_delivery_vs_estimate', 'avg_carrier_shipping_time',\n",
    "    'avg_shipping_vs_deadline'\n",
    "]].mean().reset_index()\n",
    "\n",
    "# --- Merge this final group ---\n",
    "dataframes[\"Customer Centric\"] = pd.merge(dataframes[\"Customer Centric\"], gap_features, on='customer_unique_id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c3f5b9",
   "metadata": {},
   "source": [
    "#### Feature Addition Post First Error Analysis - Purchase Intent\n",
    "- Reasoning: Some product categories are likely to be purchased repeatedly, while others aren't\n",
    "- Extraction: Find the repeat purchase rate of each product and get the max repeat purchase rate of products purchased by each customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ec2971",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Starting: Purchase Intent Feature Engineering ---\")\n",
    "\n",
    "# --- 1. Calculate Repeat Purchase Rates ---\n",
    "print(\"Calculating repeat purchase rates...\")\n",
    "customer_category_purchases = products_merged.groupby(['customer_unique_id', 'product_category_name'])['order_id'].nunique().reset_index()\n",
    "customer_category_purchases.rename(columns={'order_id': 'purchase_count'}, inplace=True)\n",
    "\n",
    "total_buyers_per_category = customer_category_purchases.groupby('product_category_name')['customer_unique_id'].nunique()\n",
    "repeat_buyers_per_category = customer_category_purchases[customer_category_purchases['purchase_count'] > 1].groupby('product_category_name')['customer_unique_id'].nunique()\n",
    "\n",
    "category_stats_df = pd.DataFrame({\n",
    "    'total_buyers': total_buyers_per_category,\n",
    "    'repeat_buyers': repeat_buyers_per_category\n",
    "})\n",
    "category_stats_df['repeat_buyers'] = category_stats_df['repeat_buyers'].fillna(0).astype(int)\n",
    "category_stats_df['repeat_purchase_rate'] = (category_stats_df['repeat_buyers'] / category_stats_df['total_buyers'])\n",
    "\n",
    "# --- 2. Filter Out Categories Without Enough Data ---\n",
    "MIN_BUYERS = 30 \n",
    "category_stats_df = category_stats_df[category_stats_df['total_buyers'] >= MIN_BUYERS]\n",
    "\n",
    "# --- 3. Define Baseline Rate and Calculate Purchase Intent Score ---\n",
    "BASELINE_RATE = 0.01\n",
    "category_stats_df['purchase_intent_score'] = category_stats_df['repeat_purchase_rate'] - BASELINE_RATE\n",
    "\n",
    "# --- 4. Map Intent to Customers ---\n",
    "category_score_map = category_stats_df['purchase_intent_score'].to_dict()\n",
    "# Map this score to every purchase in our master dataframe\n",
    "products_merged['purchase_intent_score'] = products_merged['product_category_name'].map(category_score_map)\n",
    "# Aggregate by customer, taking the MAX score\n",
    "customer_intent_df = products_merged.groupby('customer_unique_id')['purchase_intent_score'].max().reset_index()\n",
    "\n",
    "# --- 5. Merge New Feature into the Main DataFrame ---\n",
    "print(\"Merging 'purchase_intent_score' into dataframes['Customer Centric']...\")\n",
    "\n",
    "dataframes[\"Customer Centric\"] = pd.merge(\n",
    "    dataframes[\"Customer Centric\"],\n",
    "    customer_intent_df,\n",
    "    on='customer_unique_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(\"New continuous 'purchase_intent_score' feature added successfully.\")\n",
    "print(dataframes[\"Customer Centric\"]['purchase_intent_score'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627d0696",
   "metadata": {},
   "source": [
    "And we're done! Time to clean any NaNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768000dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for NaNs\n",
    "print(\"NaNs before cleaning:\")\n",
    "print(dataframes[\"Customer Centric\"].isnull().sum())\n",
    "\n",
    "# Fill NaNs with logical defaults\n",
    "dataframes[\"Customer Centric\"]['avg_review_score'] = dataframes[\"Customer Centric\"]['avg_review_score'].fillna(\n",
    "    dataframes[\"Customer Centric\"]['avg_review_score'].mean() # Impute with the mean score\n",
    ")\n",
    "dataframes[\"Customer Centric\"]['review_engagement_rate'] = dataframes[\"Customer Centric\"]['review_engagement_rate'].fillna(0) # 0% engagement\n",
    "dataframes[\"Customer Centric\"]['avg_payment_complexity'] = dataframes[\"Customer Centric\"]['avg_payment_complexity'].fillna(1) # Default to 1\n",
    "\n",
    "# Drop customer with no monetary value (never paid)\n",
    "dataframes[\"Customer Centric\"].dropna(subset=['Monetary'], inplace=True)\n",
    "\n",
    "# Handle the 2-13 gap time NaNs\n",
    "# Impute with the mean time for each\n",
    "for col in ['avg_payment_approval_time', 'avg_delivery_vs_estimate', \n",
    "            'avg_carrier_shipping_time', 'avg_shipping_vs_deadline']:\n",
    "    dataframes[\"Customer Centric\"][col] = dataframes[\"Customer Centric\"][col].fillna(\n",
    "        dataframes[\"Customer Centric\"][col].mean()\n",
    "    )\n",
    "\n",
    "# Fill final NaNs (customers with no categorized purchases) with the neutral score\n",
    "dataframes[\"Customer Centric\"]['purchase_intent_score'] = dataframes[\"Customer Centric\"]['purchase_intent_score'].fillna(0.0)\n",
    "\n",
    "print(\"NaNs after cleaning:\")\n",
    "print(dataframes[\"Customer Centric\"].isnull().sum())\n",
    "\n",
    "# Save the final file\n",
    "dataframes[\"Customer Centric\"].to_csv('customer_centric_features.csv', index=False)\n",
    "\n",
    "print(\"--- Final Customer-Centric DataFrame Assembled ---\")\n",
    "print(dataframes[\"Customer Centric\"].head())\n",
    "print(dataframes[\"Customer Centric\"].info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6544d53",
   "metadata": {},
   "source": [
    "Now we need to define what churn actually means and the feature dataframe needs to be preprocessed before using it to train the model. We'll start by defining churn as a customer having made no purchases within 180-days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5145a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, QuantileTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# --- 1. Define Target Variable (y) ---\n",
    "\n",
    "# This is a key business decision. A 180-day window is a common starting point.\n",
    "# \"If you haven't bought in the last 6 months, you are considered churned.\"\n",
    "CHURN_WINDOW_DAYS = 180\n",
    "\n",
    "dataframes[\"Customer Centric\"]['is_churned'] = (dataframes[\"Customer Centric\"]['Recency'] > CHURN_WINDOW_DAYS).astype(int)\n",
    "# (1 = Churned, 0 = Not Churned)\n",
    "\n",
    "print(f\"Churn window set to {CHURN_WINDOW_DAYS} days.\")\n",
    "print(dataframes[\"Customer Centric\"]['is_churned'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fff526",
   "metadata": {},
   "source": [
    "We'll want to train the model on a set with the same amount of churners as the test set so we'll apply stratification. The data must be split prior to preprocessing so the StandardScaler does not learn the mean and standard deviation of the test set. It is also important to drop recency so the model does not just check if recency is over 180 and ignore the other features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019dbbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Separate Features (X) and Target (y) ---\n",
    "y = dataframes[\"Customer Centric\"]['is_churned']\n",
    "X = dataframes[\"Customer Centric\"].drop(columns=['customer_unique_id', 'is_churned', 'Recency']) # Critical: Drop Recency to avoid target leakage\n",
    "\n",
    "# --- 3. Split Data with Stratification ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb81ecad",
   "metadata": {},
   "source": [
    "Numeric features need to be scaled within a normal range to avoid large values outcompeting smaller values. Preferred payment type will need to be one-hot encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec651d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Identify Numerical and Categorical Features ---\n",
    "# All columns are numerical, except for 'preferred_payment_method'\n",
    "numerical_features = X.columns.drop('preferred_payment_method')\n",
    "categorical_features = ['preferred_payment_method']\n",
    "\n",
    "print(f\"\\nIdentified {len(numerical_features)} numerical features.\")\n",
    "print(f\"Identified {len(categorical_features)} categorical features.\")\n",
    "\n",
    "# --- 5. Create a Preprocessing Pipeline ---\n",
    "# Step 1: Remove zero-variance features\n",
    "# Step 2: Apply Quantile Transformation to stabilize variance\n",
    "# Step 3: Scale numerical features\n",
    "numeric_pipeline = Pipeline(steps=[\n",
    "    ('variance_remover', VarianceThreshold(threshold=0.0)),\n",
    "    ('quantile_transform', QuantileTransformer(output_distribution='normal')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Create a OneHotEncoder for categorical features\n",
    "# handle_unknown='ignore' prevents errors if new categories appear in test data\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# Use ColumnTransformer to apply transformers to the correct columns\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_pipeline, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough' # Keep any columns not listed (just in case)\n",
    ")\n",
    "\n",
    "# --- 6. Apply Preprocessing (The Correct Way) ---\n",
    "# Fit and transform *only* the training data\n",
    "print(\"\\nFitting preprocessor on X_train...\")\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "\n",
    "# --- DEBUGGING CHECKS ---\n",
    "print(\"--- Checking for NaNs and Infs after preprocessing ---\")\n",
    "\n",
    "# Check for NaNs\n",
    "nan_count = np.isnan(X_train_processed).sum()\n",
    "print(f\"Total NaNs created: {nan_count}\")\n",
    "\n",
    "# Check for Infs\n",
    "inf_count = np.isinf(X_train_processed).sum()\n",
    "print(f\"Total Infs created: {inf_count}\")\n",
    "\n",
    "if nan_count > 0 or inf_count > 0:\n",
    "    print(\"\\nWARNING: Your preprocessing pipeline is creating bad values!\")\n",
    "else:\n",
    "    print(\"\\nNo NaNs or Infs found. The pipeline is clean.\")\n",
    "    # If clean, print the max values of each column\n",
    "    print(\"\\nMax value in each processed column:\")\n",
    "    print(X_train_processed.max(axis=0))\n",
    "\n",
    "# *Only transform* the test data (using rules learned from train)\n",
    "print(\"Transforming X_test...\")\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "print(\"Preprocessing complete.\")\n",
    "print(f\"X_train_processed shape: {X_train_processed.shape}\")\n",
    "print(f\"X_test_processed shape: {X_test_processed.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e19c10",
   "metadata": {},
   "source": [
    "The training and test data is ready to go, time to train a few models! We'll start wtih logistic regression for a quick and easy baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980fa2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "model_results = {}\n",
    "\n",
    "# --- 1. Initialize and Train ---\n",
    "print(\"--- Training: Logistic Regression ---\")\n",
    "lr_model = LogisticRegression(class_weight='balanced', random_state=42, max_iter=1000, solver='liblinear')\n",
    "lr_model.fit(X_train_processed, y_train)\n",
    "\n",
    "# --- 2. Get Predictions ---\n",
    "y_pred_lr = lr_model.predict(X_test_processed)\n",
    "y_prob_lr = lr_model.predict_proba(X_test_processed)[:, 1] # Get probabilities for the \"churn\" class\n",
    "\n",
    "# --- 3. Evaluate ---\n",
    "print(\"\\n--- Logistic Regression Results ---\")\n",
    "print(classification_report(y_test, y_pred_lr, target_names=['Not Churned', 'Churned']))\n",
    "\n",
    "roc_auc_lr = roc_auc_score(y_test, y_prob_lr)\n",
    "print(f\"ROC-AUC Score: {roc_auc_lr:.4f}\")\n",
    "\n",
    "model_results['Logistic Regression'] = {\n",
    "    \"ROC-AUC\": roc_auc_lr,\n",
    "    \"Recall (Churn)\": recall_score(y_test, y_pred_lr, pos_label=1),\n",
    "    \"Precision (Churn)\": precision_score(y_test, y_pred_lr, pos_label=1)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292472d9",
   "metadata": {},
   "source": [
    "Despite the strange and persistent numerical errors, the performance is not too bad. Now let's try a random forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a1aee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "print(\"--- Training: RandomForestClassifier ---\")\n",
    "\n",
    "rf_model = RandomForestClassifier(\n",
    "    class_weight='balanced', \n",
    "    random_state=42, \n",
    "    n_estimators=100, # A good default\n",
    "    n_jobs=-1 # Use all available cores\n",
    ")\n",
    "\n",
    "rf_model.fit(X_train_processed, y_train)\n",
    "\n",
    "# --- Evaluate ---\n",
    "y_pred_rf = rf_model.predict(X_test_processed)\n",
    "y_prob_rf = rf_model.predict_proba(X_test_processed)[:, 1]\n",
    "\n",
    "print(\"\\n--- RandomForestClassifier Results ---\")\n",
    "print(classification_report(y_test, y_pred_rf, target_names=['Not Churned', 'Churned']))\n",
    "roc_auc_rf = roc_auc_score(y_test, y_prob_rf)\n",
    "print(f\"ROC-AUC Score: {roc_auc_rf:.4f}\")\n",
    "\n",
    "model_results['RandomForestClassifier'] = {\n",
    "    \"ROC-AUC\": roc_auc_rf,\n",
    "    \"Recall (Churn)\": recall_score(y_test, y_pred_rf, pos_label=1),\n",
    "    \"Precision (Churn)\": precision_score(y_test, y_pred_rf, pos_label=1)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7433a4bf",
   "metadata": {},
   "source": [
    "That is great performance! Let's try a gradient boosting model next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4e9186",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import classification_report, roc_auc_score, recall_score, precision_score\n",
    "\n",
    "print(\"--- Training: XGBoost Classifier ---\")\n",
    "\n",
    "# --- 1. Calculate scale_pos_weight for imbalance ---\n",
    "# This is the XGBoost equivalent of class_weight='balanced'\n",
    "# (Count of negative class) / (Count of positive class)\n",
    "weight_ratio = y_train.value_counts()[0] / y_train.value_counts()[1]\n",
    "print(f\"Calculated scale_pos_weight: {weight_ratio:.2f}\")\n",
    "\n",
    "\n",
    "# --- 2. Initialize and Train ---\n",
    "# 'n_jobs=-1' uses all your CPU cores\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    scale_pos_weight=weight_ratio,\n",
    "    random_state=42,\n",
    "    n_jobs=-1 \n",
    ")\n",
    "\n",
    "xgb_model.fit(X_train_processed, y_train)\n",
    "\n",
    "# --- 3. Get Predictions ---\n",
    "y_pred_xgb = xgb_model.predict(X_test_processed)\n",
    "y_prob_xgb = xgb_model.predict_proba(X_test_processed)[:, 1]\n",
    "\n",
    "# --- 4. Evaluate ---\n",
    "print(\"\\n--- XGBoost Classifier Results ---\")\n",
    "print(classification_report(y_test, y_pred_xgb, target_names=['Not Churned', 'Churned']))\n",
    "\n",
    "roc_auc_xgb = roc_auc_score(y_test, y_prob_xgb)\n",
    "print(f\"ROC-AUC Score: {roc_auc_xgb:.4f}\")\n",
    "\n",
    "model_results['XGBoost'] = {\n",
    "    \"ROC-AUC\": roc_auc_xgb,\n",
    "    \"Recall (Churn)\": recall_score(y_test, y_pred_xgb, pos_label=1),\n",
    "    \"Precision (Churn)\": precision_score(y_test, y_pred_xgb, pos_label=1)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a41473",
   "metadata": {},
   "source": [
    "Excellent score! The feature engineering and preprocessing pipeline have paid off. The next step is hyperparameter tuning. First, we'll use GridSearchCV to find the best combination of architecture hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aee4252",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# --- Setup (assumes X_train_processed, y_train exist) ---\n",
    "weight_ratio = y_train.value_counts()[0] / y_train.value_counts()[1]\n",
    "print(f\"Using scale_pos_weight: {weight_ratio:.2f}\")\n",
    "\n",
    "# --- STEP 1: Tune Architecture Hyperparameters ---\n",
    "print(\"\\n--- Starting Step 1: Tuning Model Architecture ---\")\n",
    "\n",
    "# Your 4x4x4x4x4 grid = 1024 combinations\n",
    "param_grid_1 = {\n",
    "    'n_estimators': [100, 200, 300, 500],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7, 10],\n",
    "    'subsample': [0.7, 0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.7, 0.8, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "# Base estimator for the grid search\n",
    "base_estimator = xgb.XGBClassifier(\n",
    "    scale_pos_weight=weight_ratio, \n",
    "    random_state=42, \n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Setup the GridSearchCV\n",
    "# cv=5 is standard, n_jobs=-1 uses all cores for the search\n",
    "grid_search_1 = GridSearchCV(\n",
    "    estimator=base_estimator,\n",
    "    param_grid=param_grid_1,\n",
    "    cv=5,\n",
    "    scoring='roc_auc',\n",
    "    verbose=2, # This will print updates as it runs\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "\n",
    "grid_search_1.fit(X_train_processed, y_train)\n",
    "\n",
    "print(f\"Best ROC-AUC from Step 1: {grid_search_1.best_score_:.4f}\")\n",
    "print(\"Best Architecture Parameters found:\")\n",
    "print(grid_search_1.best_params_)\n",
    "\n",
    "# Store the best params for Step 2\n",
    "step_1_best_params = grid_search_1.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fb76cf",
   "metadata": {},
   "source": [
    "Now we'll keep those architecure params and find the best combination of regularization params."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ccc3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- STEP 2: Tune Regularization Hyperparameters ---\n",
    "print(\"\\n--- Starting Step 2: Tuning Regularization ---\")\n",
    "\n",
    "# Define the new grid for regularization\n",
    "param_grid_2 = {\n",
    "    'gamma': [0, 0.1, 0.5, 1, 2],\n",
    "    'reg_alpha': [0, 0.01, 0.1, 1],\n",
    "    'reg_lambda': [0.1, 0.5, 1, 2, 5]\n",
    "}\n",
    "\n",
    "# **CRITICAL:** Create a new estimator that *already has*\n",
    "# the best parameters from Step 1.\n",
    "# We use **step_1_best_params to \"unpack\" the dictionary\n",
    "# of best settings we just found.\n",
    "fine_tune_estimator = xgb.XGBClassifier(\n",
    "    **step_1_best_params,  # Unpacks n_estimators, max_depth, etc.\n",
    "    scale_pos_weight=weight_ratio, \n",
    "    random_state=42, \n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Setup the *second* GridSearchCV\n",
    "grid_search_2 = GridSearchCV(\n",
    "    estimator=fine_tune_estimator,\n",
    "    param_grid=param_grid_2,\n",
    "    cv=5,\n",
    "    scoring='roc_auc',\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search_2.fit(X_train_processed, y_train)\n",
    "\n",
    "print(f\"Final Best ROC-AUC: {grid_search_2.best_score_:.4f}\")\n",
    "print(\"Final Best Regularization Parameters found:\")\n",
    "print(grid_search_2.best_params_)\n",
    "\n",
    "# --- FINAL MODEL ---\n",
    "# This is your fully optimized model\n",
    "final_best_model = grid_search_2.best_estimator_\n",
    "\n",
    "# You can now use this model to evaluate on your test set\n",
    "y_pred_final = final_best_model.predict(X_test_processed)\n",
    "y_prob_final = final_best_model.predict_proba(X_test_processed)[:, 1]\n",
    "\n",
    "print(\"\\n--- Final Tuned Model Results on Test Set ---\")\n",
    "print(classification_report(y_test, y_pred_final, target_names=['Not Churned', 'Churned']))\n",
    "print(f\"Final ROC-AUC Score: {roc_auc_score(y_test, y_prob_final):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15dae8e9",
   "metadata": {},
   "source": [
    "The final performace is fantasic! Let's save it so we don't have to run another grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0d3ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# 1. Save the final model\n",
    "joblib.dump(final_best_model, 'final_churn_model.joblib')\n",
    "\n",
    "# 2. Save the preprocessor (from cell 85)\n",
    "joblib.dump(preprocessor, 'churn_preprocessor.joblib')\n",
    "\n",
    "print(\"\\nSuccessfully saved 'final_churn_model.joblib' and 'churn_preprocessor.joblib'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe13363",
   "metadata": {},
   "source": [
    "Now that the model is fine-tuned and powerful, we can run feature importance to establish which features are driving these accurate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1689ffd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Get Feature Names ---\n",
    "try:\n",
    "    num_features = numerical_features.tolist() \n",
    "    cat_features = preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features).tolist()\n",
    "    all_feature_names = num_features + cat_features\n",
    "    print(f\"Retrieved {len(all_feature_names)} feature names.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: Make sure 'preprocessor', 'numerical_features', and 'categorical_features' are defined.\")\n",
    "\n",
    "# --- 2. Get Importances from your FINAL model ---\n",
    "importances = final_best_model.feature_importances_ \n",
    "\n",
    "# --- 3. Create, Sort, and Print DataFrame ---\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': all_feature_names,\n",
    "    'Importance': importances\n",
    "})\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(\"\\n--- Final Model Feature Importances ---\")\n",
    "print(importance_df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6282590b",
   "metadata": {},
   "source": [
    "### Key Insights\n",
    " - The most impactful features are mostly logistics related rather than product related. This shows a need to focus on logistics optimization to find cheaper, faster, and more reliable shipping partners and improve delivery estimates. \n",
    " \n",
    " - Payment method is also a huge predictor, revealing that debit card, boleto, and voucher shoppers are high-churn, though debit card and boleto are highest. This suggests a need to target debit card and boleto shoppers with discount offers immediately after their first purchase to build a habit.\n",
    "\n",
    " - Classic loyalty metrics (Monetary, Tenure, Frequency) are expectedly all high in the ranking.\n",
    "\n",
    " - Average review score and review engagement rate are surprisingly unimportant.\n",
    "\n",
    " - The new purchase_intent_score was somewhat helpful.\n",
    "\n",
    " ### High-Risk Profile\n",
    " - Based on this data a high-churn-risk customer is one who paid a high shipping fee for their order, used Boleto or a Debit Card to pay, and has only purchased once or twice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824ba410",
   "metadata": {},
   "source": [
    "### Error Analysis - What is throwing the model off?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee13b319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Get the ORIGINAL, UN-PROCESSED Test Data ---\n",
    "# We use the index from y_test to select the original test rows from 'X'\n",
    "X_test_original = X.loc[y_test.index]\n",
    "\n",
    "# --- 2. Create the Analysis DataFrame ---\n",
    "# Combine the original features, the true labels, and our model's predictions\n",
    "error_analysis_df = X_test_original.copy()\n",
    "error_analysis_df['actual_churn'] = y_test\n",
    "error_analysis_df['predicted_churn'] = y_pred_final\n",
    "\n",
    "# --- 3. Isolate the Mistakes ---\n",
    "\n",
    "# False Negatives (FN): Model said 0, but it was 1. (THE \"MISSED\" ONES)\n",
    "fn_df = error_analysis_df[\n",
    "    (error_analysis_df['predicted_churn'] == 0) & \n",
    "    (error_analysis_df['actual_churn'] == 1)\n",
    "]\n",
    "\n",
    "# False Positives (FP): Model said 1, but it was 0. (THE \"FALSE ALARMS\")\n",
    "fp_df = error_analysis_df[\n",
    "    (error_analysis_df['predicted_churn'] == 1) & \n",
    "    (error_analysis_df['actual_churn'] == 0)\n",
    "]\n",
    "\n",
    "# --- 4. Get Statistical Summaries of the Mistakes ---\n",
    "\n",
    "print(f\"Total Test Samples: {len(error_analysis_df)}\")\n",
    "print(f\"Total False Negatives (Missed Churn): {len(fn_df)}\")\n",
    "print(f\"Total False Positives (False Alarms): {len(fp_df)}\")\n",
    "\n",
    "print(\"\\n--- Profile of MISSED Churners (False Negatives) ---\")\n",
    "print(fn_df.describe())\n",
    "\n",
    "print(\"\\n--- Profile of LOYAL Customers We Flagged (False Positives) ---\")\n",
    "print(fp_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b835fca",
   "metadata": {},
   "source": [
    "### Interpretation of Analysis - First Model\n",
    "These profiles are strikingly similar, suggesting there is a missing feature the model needs to consider. Maybe some product categories are more likely to be repeatedly purchased. It is worth exploring the dataset to see what categories are purchased repeatedly.\n",
    "\n",
    "### Interpretation of Analysis - Second Model\n",
    "The profiles are still about the same. The churn is likely based on factors outside of the data like customer service, website experience, returns, or involuntary churn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093d778f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"--- Starting: Visual Error Analysis ---\")\n",
    "\n",
    "# List of key features to compare\n",
    "# These are your most important features, plus our 'happy churner' hypotheses\n",
    "features_to_plot = [\n",
    "    'avg_shipping_cost',       # Your #1 most important feature\n",
    "    'Monetary',\n",
    "    'avg_carrier_shipping_time',\n",
    "    'Tenure',\n",
    "    'avg_review_score',        # Key to the 'happy churner' theory\n",
    "    'avg_delivery_vs_estimate',\n",
    "    'purchase_intent_score'    # Our new feature\n",
    "]\n",
    "\n",
    "# --- Create the plots ---\n",
    "# This creates a grid of plots, 2 columns wide\n",
    "num_plots = len(features_to_plot)\n",
    "num_cols = 2\n",
    "num_rows = (num_plots + 1) // num_cols # (calculates rows needed)\n",
    "\n",
    "plt.figure(figsize=(12, num_rows * 4)) # Make the figure tall enough\n",
    "\n",
    "for i, feature in enumerate(features_to_plot):\n",
    "    plt.subplot(num_rows, num_cols, i + 1) # Create a new subplot\n",
    "    \n",
    "    # Plot the distributions\n",
    "    # 'kde' = Kernel Density Estimate (a smooth histogram)\n",
    "    sns.kdeplot(fn_df[feature], label='Missed Churners (FN)', color='blue', fill=True)\n",
    "    sns.kdeplot(fp_df[feature], label='False Alarms (FP)', color='red', fill=True)\n",
    "    \n",
    "    plt.title(f'Distribution of \"{feature}\" for Error Groups')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout() # Prevents plots from overlapping\n",
    "plt.savefig('error_analysis_distributions.png')\n",
    "\n",
    "print(\"--- Visual Error Analysis Complete ---\")\n",
    "print(\"Saved plots to 'error_analysis_distributions.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e4e0b0",
   "metadata": {},
   "source": [
    "### Final Thoughts\n",
    "The overlapping graphs in this final visual error analysis suggests that we've hit the point of diminishing returns. The features we have available are just not useful in telling these groups apart, there is some outside ambiguity. However, I'm incredibly satisfied with the models performance. A ROC-AUC of 0.9418 indicates a powerful, highly performant model so it's time to move on to the longitudinal model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
